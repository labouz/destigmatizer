{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reframe Package Example Notebook\n",
    "\n",
    "This notebook demonstrates the core functionality of the Reframe package for analyzing and rewriting drug-related and stigmatizing content.\n",
    "\n",
    "## Features demonstrated:\n",
    "1. Classifying drug-related content\n",
    "2. Identifying stigmatizing language\n",
    "3. Analyzing text style\n",
    "4. Rewriting stigmatizing content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the reframe package and its components\n",
    "import reframe\n",
    "from reframe import core\n",
    "from reframe.clients import get_client\n",
    "from reframe.utils import get_default_model, get_model_mapping\n",
    "\n",
    "# Define example texts for demonstration\n",
    "NON_DRUG_POST = \"I think we should really work on the housing crisis in urban areas. The homeless are getting scary.\"\n",
    "DRUG_NON_STIGMA_POST = \"I really feel for people who suffer from substance use disorder, being unable to control an impulse due to dependency sounds scary.\"\n",
    "DRUG_AND_STIGMA_POST = \"Addicts really need to get control of themselves. Just stop doing drugs. It seems like these people are just lacking willpower.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup API Client\n",
    "\n",
    "First, we need to initialize the client with your API key. This example uses OpenAI, but you could use other clients as supported by the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client initialized with model: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "# Initialize with your API key\n",
    "# You can either load from a secrets file or directly input your key\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Option 1: Load from secrets file\n",
    "try:\n",
    "    with open('./secrets.json') as f:\n",
    "        secrets = json.load(f)\n",
    "    api_key = secrets.get('OPENAI_API_KEY')\n",
    "except (FileNotFoundError, json.JSONDecodeError, KeyError):\n",
    "    # Option 2: Get from environment variable\n",
    "    api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize client\n",
    "client = get_client('openai', api_key=api_key)\n",
    "model = get_default_model('openai')  # Get default model for this client type\n",
    "\n",
    "print(f\"Client initialized with model: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Texts\n",
    "\n",
    "Let's take a look at our example texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NON_DRUG_POST:\n",
      "I think we should really work on the housing crisis in urban areas. The homeless are getting scary.\n",
      "\n",
      "DRUG_NON_STIGMA_POST:\n",
      "I really feel for people who suffer from substance use disorder, being unable to control an impulse due to dependency sounds scary.\n",
      "\n",
      "DRUG_AND_STIGMA_POST:\n",
      "Addicts really need to get control of themselves. Just stop doing drugs. It seems like these people are just lacking willpower.\n"
     ]
    }
   ],
   "source": [
    "# Display the example texts\n",
    "print(\"NON_DRUG_POST:\")\n",
    "print(NON_DRUG_POST)\n",
    "print(\"\\nDRUG_NON_STIGMA_POST:\")\n",
    "print(DRUG_NON_STIGMA_POST)\n",
    "print(\"\\nDRUG_AND_STIGMA_POST:\")\n",
    "print(DRUG_AND_STIGMA_POST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classifying Drug-Related Content\n",
    "\n",
    "Let's classify texts to determine if they're related to drugs or substance use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying drug content for: \"I think we should really work on the housing crisi...\"\n",
      "Drug classification result: nd\n",
      "\n",
      "Classifying drug content for: \"Addicts really need to get control of themselves. ...\"\n",
      "Drug classification result: d\n",
      "\n",
      "Classifying drug content for: \"I really feel for people who suffer from substance...\"\n",
      "Drug classification result: nd\n"
     ]
    }
   ],
   "source": [
    "# Classify the non-drug post\n",
    "print(f\"Classifying drug content for: \\\"{NON_DRUG_POST[:50]}...\\\"\")\n",
    "non_drug_result = core.classify_if_drug(text=NON_DRUG_POST, client=client, model=model)\n",
    "print(f\"Drug classification result: {non_drug_result}\")\n",
    "\n",
    "# Classify the drug and stigma post\n",
    "print(f\"\\nClassifying drug content for: \\\"{DRUG_AND_STIGMA_POST[:50]}...\\\"\")\n",
    "drug_stigma_result = core.classify_if_drug(text=DRUG_AND_STIGMA_POST, client=client, model=model)\n",
    "print(f\"Drug classification result: {drug_stigma_result}\")\n",
    "\n",
    "# Classify the drug non-stigma post\n",
    "print(f\"\\nClassifying drug content for: \\\"{DRUG_NON_STIGMA_POST[:50]}...\\\"\")\n",
    "drug_non_stigma_result = core.classify_if_drug(text=DRUG_NON_STIGMA_POST, client=client, model=model)\n",
    "print(f\"Drug classification result: {drug_non_stigma_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Identifying Stigmatizing Language\n",
    "\n",
    "Next, let's identify if texts contain stigmatizing language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying stigmatizing language for: \"I really feel for people who suffer from substance...\"\n",
      "Stigma classification result: ns\n",
      "\n",
      "Classifying stigmatizing language for: \"Addicts really need to get control of themselves. ...\"\n",
      "Stigma classification result: s, labeling: uses the term 'addicts' in a derogatory manner, stereotyping: assumes that people with addiction simply lack willpower, ignoring the complexity of substance use disorders, separation: implies a moral failing, creating a divide between those who use drugs and those who don't, discrimination: suggests that people with addiction are responsible for their condition and undeserving of empathy or support.\n",
      "\n",
      "Classifying stigmatizing language for: \"I think we should really work on the housing crisi...\"\n",
      "Stigma classification result: s, labeling: refers to homeless individuals in a way that implies they are inherently frightening, stereotyping: implies that homeless people, who may include those with substance use disorders, are dangerous, separation: creates a divide between housed individuals and those experiencing homelessness, discrimination: suggests that homeless individuals are a problem to be managed rather than people in need of support and resources.\n"
     ]
    }
   ],
   "source": [
    "# Classify the drug non-stigma post\n",
    "print(f\"Classifying stigmatizing language for: \\\"{DRUG_NON_STIGMA_POST[:50]}...\\\"\")\n",
    "non_stigma_result = core.classify_if_stigma(text=DRUG_NON_STIGMA_POST, client=client, model=model)\n",
    "print(f\"Stigma classification result: {non_stigma_result}\")\n",
    "\n",
    "# Classify the drug and stigma post\n",
    "print(f\"\\nClassifying stigmatizing language for: \\\"{DRUG_AND_STIGMA_POST[:50]}...\\\"\")\n",
    "stigma_result = core.classify_if_stigma(text=DRUG_AND_STIGMA_POST, client=client, model=model)\n",
    "print(f\"Stigma classification result: {stigma_result}\")\n",
    "\n",
    "# Classify the non-drug post\n",
    "print(f\"\\nClassifying stigmatizing language for: \\\"{NON_DRUG_POST[:50]}...\\\"\")\n",
    "non_drug_stigma_result = core.classify_if_stigma(text=NON_DRUG_POST, client=client, model=model)\n",
    "print(f\"Stigma classification result: {non_drug_stigma_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Combined Classification\n",
    "\n",
    "Now let's classify for both drug content and stigmatizing language together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing text for drug content and stigma: \"Addicts really need to get control of themselves. ...\"\n",
      "Drug classification: d\n",
      "Stigma classification: s, labeling: uses the term 'addicts' in a derogatory manner, stereotyping: assumes that people with addiction simply lack willpower, ignoring the complexity of substance use disorders, separation: implies a moral failing, creating a divide between those who use drugs and those who don't, discrimination: suggests that people with addiction are responsible for their condition and undeserving of empathy or support.\n",
      "Combined result: (d, s, labeling: uses the term 'addicts' in a derogatory manner, stereotyping: assumes that people with addiction simply lack willpower, ignoring the complexity of substance use disorders, separation: implies a moral failing, creating a divide between those who use drugs and those who don't, discrimination: suggests that people with addiction are responsible for their condition and undeserving of empathy or support.)\n",
      "\n",
      "Analyzing text for drug content and stigma: \"I really feel for people who suffer from substance...\"\n",
      "Drug classification: nd\n",
      "Stigma classification: ns\n",
      "Combined result: (nd, ns)\n",
      "\n",
      "Analyzing text for drug content and stigma: \"I think we should really work on the housing crisi...\"\n",
      "Drug classification: nd\n",
      "Stigma classification: s, labeling: refers to homeless individuals in a way that implies they are inherently frightening, stereotyping: implies that homeless people, who may include those with substance use disorders, are dangerous, separation: creates a divide between housed individuals and those experiencing homelessness, discrimination: suggests that homeless individuals are a problem to be managed rather than people in need of support and resources.\n",
      "Combined result: (nd, s, labeling: refers to homeless individuals in a way that implies they are inherently frightening, stereotyping: implies that homeless people, who may include those with substance use disorders, are dangerous, separation: creates a divide between housed individuals and those experiencing homelessness, discrimination: suggests that homeless individuals are a problem to be managed rather than people in need of support and resources.)\n"
     ]
    }
   ],
   "source": [
    "# Perform combined classification on the drug and stigma post\n",
    "print(f\"Analyzing text for drug content and stigma: \\\"{DRUG_AND_STIGMA_POST[:50]}...\\\"\")\n",
    "\n",
    "drug_result = core.classify_if_drug(text=DRUG_AND_STIGMA_POST, client=client, model=model)\n",
    "stigma_result = core.classify_if_stigma(text=DRUG_AND_STIGMA_POST, client=client, model=model)\n",
    "\n",
    "print(f\"Drug classification: {drug_result}\")\n",
    "print(f\"Stigma classification: {stigma_result}\")\n",
    "print(f\"Combined result: ({drug_result}, {stigma_result})\")\n",
    "\n",
    "# Let's analyze the non-stigma post\n",
    "print(f\"\\nAnalyzing text for drug content and stigma: \\\"{DRUG_NON_STIGMA_POST[:50]}...\\\"\")\n",
    "drug_result2 = core.classify_if_drug(text=DRUG_NON_STIGMA_POST, client=client, model=model)\n",
    "stigma_result2 = core.classify_if_stigma(text=DRUG_NON_STIGMA_POST, client=client, model=model)\n",
    "print(f\"Drug classification: {drug_result2}\")\n",
    "print(f\"Stigma classification: {stigma_result2}\")\n",
    "print(f\"Combined result: ({drug_result2}, {stigma_result2})\")\n",
    "\n",
    "# Let's analyze the non-drug post\n",
    "print(f\"\\nAnalyzing text for drug content and stigma: \\\"{NON_DRUG_POST[:50]}...\\\"\")\n",
    "drug_result3 = core.classify_if_drug(text=NON_DRUG_POST, client=client, model=model)\n",
    "stigma_result3 = core.classify_if_stigma(text=NON_DRUG_POST, client=client, model=model)\n",
    "print(f\"Drug classification: {drug_result3}\")\n",
    "print(f\"Stigma classification: {stigma_result3}\")\n",
    "print(f\"Combined result: ({drug_result3}, {stigma_result3})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Style Analysis\n",
    "\n",
    "Before rewriting, we can analyze the style of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text style analysis for example stigmatizing post:\n",
      "{'punctuation_usage': 'moderate, with . being most frequent', 'passive_voice_usage': 'none', 'sentence_length_variation': 'ranging from short (4 words) to long (9 words) with an average of 7.0 words per sentence', 'lexical_diversity': '123.48 (MTLD)', 'top_emotions': 'judgmental'}\n",
      "\n",
      "Text style analysis for non-stigmatizing post:\n",
      "{'punctuation_usage': 'moderate, with ,, . being most frequent', 'passive_voice_usage': 'none', 'sentence_length_variation': 'ranging from short (22 words) to long (22 words) with an average of 22.0 words per sentence', 'lexical_diversity': '135.52 (MTLD)', 'top_emotions': 'compassion'}\n"
     ]
    }
   ],
   "source": [
    "# Analyze the style of the example stigmatizing post\n",
    "example_style = core.analyze_text_llm(text=DRUG_AND_STIGMA_POST, client=client, model=model)\n",
    "print(f\"Text style analysis for example stigmatizing post:\\n{example_style}\")\n",
    "\n",
    "# Also analyze the non-stigma post\n",
    "non_stigma_style = core.analyze_text_llm(text=DRUG_NON_STIGMA_POST, client=client, model=model)\n",
    "print(f\"\\nText style analysis for non-stigmatizing post:\\n{non_stigma_style}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Rewriting Stigmatizing Content\n",
    "\n",
    "Let's rewrite stigmatizing content to be more neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Addicts really need to get control of themselves. Just stop doing drugs. It seems like these people are just lacking willpower.\n",
      "Rewritten: individuals struggling with substance use need support and understanding. overcoming substance use is challenging. it requires compassion and empathy.\n"
     ]
    }
   ],
   "source": [
    "# Get stigma classification for the example post\n",
    "example_post = DRUG_AND_STIGMA_POST\n",
    "example_stigma = core.classify_if_stigma(text=example_post, client=client, model=model)\n",
    "\n",
    "# Get text style for rewriting\n",
    "example_style_instruct = str(core.analyze_text_llm(text=example_post, client=client, model=model))\n",
    "\n",
    "# Rewrite the stigmatizing content\n",
    "rewritten_example = core.rewrite_to_destigma(\n",
    "    text=example_post,\n",
    "    explanation=example_stigma,\n",
    "    style_instruct=example_style_instruct,\n",
    "    model=model,\n",
    "    client=client\n",
    ")\n",
    "\n",
    "print(f\"Original: {example_post}\")\n",
    "print(f\"Rewritten: {rewritten_example}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Workflow Encapsulation\n",
    "\n",
    "Finally, you can also see how all the functions work together in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Classifying drug-related content...\n",
      "Text is not drug-related. Skipping further analysis.\n",
      "Original: Addicts really need to get control of themselves. Just stop doing drugs. It seems like these people are just lacking willpower.\n",
      "Rewritten: Addicts really need to get control of themselves. Just stop doing drugs. It seems like these people are just lacking willpower.\n"
     ]
    }
   ],
   "source": [
    "rewrote_text = core.analyze_and_rewrite_text(DRUG_AND_STIGMA_POST, client, model)\n",
    "print(f\"Original: {DRUG_AND_STIGMA_POST}\")\n",
    "print(f\"Rewritten: {rewrote_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've demonstrated the core functionality of the Reframe package:\n",
    "1. Classifying drug-related content\n",
    "2. Identifying stigmatizing language\n",
    "3. Analyzing text style\n",
    "4. Rewriting stigmatizing content to be more neutral while preserving the original message\n",
    "\n",
    "These tools can be used to analyze and improve communication about substance use disorders and reduce harmful stigma."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reframe-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
